import os.path as osp
from functools import partial
from typing import Type, Any, Callable, Union, List, Optional

import torch
import torch.nn as nn
from torch import Tensor
import torch.nn.functional as F

import torch_geometric.transforms as T
from torch_geometric.nn import MLP, fps, global_max_pool, global_mean_pool, radius

from modules import *
 
### Models from CDConv: https://github.com/hehefan/Continuous-Discrete-Convolution

class Linear(nn.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 batch_norm: bool = True,
                 dropout: float = 0.0,
                 bias: bool = False,
                 leakyrelu_negative_slope: float = 0.1,
                 momentum: float = 0.2) -> nn.Module:
        super(Linear, self).__init__()

        module = []
        if batch_norm:
            module.append(nn.BatchNorm1d(in_channels, momentum=momentum))
        module.append(nn.LeakyReLU(leakyrelu_negative_slope))
        module.append(nn.Dropout(dropout))
        module.append(nn.Linear(in_channels, out_channels, bias = bias))
        self.module = nn.Sequential(*module)

    def forward(self, x):
        return self.module(x)

class MLP(nn.Module):
    def __init__(self,
                 in_channels: int,
                 mid_channels: int,
                 out_channels: int,
                 batch_norm: bool,
                 dropout: float = 0.0,
                 bias: bool = True,
                 leakyrelu_negative_slope: float = 0.2,
                 momentum: float = 0.2) -> nn.Module:
        super(MLP, self).__init__()

        module = []
        if batch_norm:
            module.append(nn.BatchNorm1d(in_channels, momentum=momentum))
        module.append(nn.LeakyReLU(leakyrelu_negative_slope))
        module.append(nn.Dropout(dropout))
        if mid_channels is None:
            module.append(nn.Linear(in_channels, out_channels, bias = bias))
        else:
            module.append(nn.Linear(in_channels, mid_channels, bias = bias))
        if batch_norm:
            if mid_channels is None:
                module.append(nn.BatchNorm1d(out_channels, momentum=momentum))
            else:
                module.append(nn.BatchNorm1d(mid_channels, momentum=momentum))
        module.append(nn.LeakyReLU(leakyrelu_negative_slope))
        if mid_channels is None:
            module.append(nn.Dropout(dropout))
        else:
            module.append(nn.Linear(mid_channels, out_channels, bias = bias))

        self.module = nn.Sequential(*module)

    def forward(self, input):
        return self.module(input)

class BasicBlock(nn.Module):
    def __init__(self,
                 r: float,
                 l: float,
                 kernel_channels: List[int],
                 in_channels: int,
                 out_channels: int,
                 base_width: float = 16.0,
                 batch_norm: bool = True,
                 dropout: float = 0.0,
                 bias: bool = False,
                 leakyrelu_negative_slope: float = 0.1,
                 momentum: float = 0.2) -> nn.Module:

        super(BasicBlock, self).__init__()

        if in_channels != out_channels:
            self.identity = Linear(in_channels=in_channels,
                                  out_channels=out_channels,
                                  batch_norm=batch_norm,
                                  dropout=dropout,
                                  bias=bias,
                                  leakyrelu_negative_slope=leakyrelu_negative_slope,
                                  momentum=momentum)
        else:
            self.identity = nn.Sequential()

        width = int(out_channels * (base_width / 64.))
        self.input = MLP(in_channels=in_channels,
                         mid_channels=None,
                         out_channels=width,
                         batch_norm=batch_norm,
                         dropout=dropout,
                         bias=bias,
                         leakyrelu_negative_slope=leakyrelu_negative_slope,
                         momentum=momentum)
        self.conv = CDConv_Model(r=r, l=l, kernel_channels=kernel_channels, in_channels=width, out_channels=width)
        self.output = Linear(in_channels=width,
                             out_channels=out_channels,
                             batch_norm=batch_norm,
                             dropout=dropout,
                             bias=bias,
                             leakyrelu_negative_slope=leakyrelu_negative_slope,
                             momentum=momentum)

    def forward(self, x, pos, seq, ori, batch):    #, pos_n, pos_cb):
        identity = self.identity(x)
        x = self.input(x)
        # x = self.conv(x, pos, seq, ori, batch, pos_n, pos_cb)
        x = self.conv(x, pos, seq, ori, batch)
        out = self.output(x) + identity
        return out

class Model(nn.Module):
    def __init__(self,
                 geometric_radii: List[float],
                 sequential_kernel_size: float,
                 kernel_channels: List[int],
                 channels: List[int],
                 base_width: float = 16.0,
                 embedding_dim: int = 16,
                 batch_norm: bool = True,
                 dropout: float = 0.2,
                 bias: bool = False,
                 num_classes: int = 384) -> nn.Module:
        super().__init__()

        assert len(geometric_radii) == len(channels), \
            "Model: 'geometric_radii' and 'channels' should have the same number of elements!"

        self.embedding = nn.Embedding(num_embeddings=21, embedding_dim=embedding_dim)
        self.local_mean_pool = AvgPooling()
        self.local_mean_pool1 = AvgPooling1()

        # in_channels = embedding_dim + 8 + 6 + 7  # embedding + side_chain_embs + bb_embs + pc7
        in_channels = embedding_dim + 8 + 6 
        layers = []

        for radius, out_channels in zip(geometric_radii, channels):
            # Add two BasicBlocks per resolution
            layers.append(BasicBlock(
                r=radius,
                l=sequential_kernel_size,
                kernel_channels=kernel_channels,
                in_channels=in_channels,
                out_channels=out_channels,
                base_width=base_width,
                batch_norm=batch_norm,
                dropout=dropout,
                bias=bias
            ))
            layers.append(BasicBlock(
                r=radius,
                l=sequential_kernel_size,
                kernel_channels=kernel_channels,
                in_channels=out_channels,
                out_channels=out_channels,
                base_width=base_width,
                batch_norm=batch_norm,
                dropout=dropout,
                bias=bias
            ))
            in_channels = out_channels  # Update for next block

        self.layers = nn.Sequential(*layers)

        self.classifier = MLP(
            in_channels=channels[-1],
            mid_channels=max(channels[-1], num_classes),
            out_channels=num_classes,
            batch_norm=batch_norm,
            dropout=dropout
        )

    def forward(self, data):
        # Unpack input
        x = self.embedding(data.x)
        # x = torch.cat([x, data.side_chain_embs, data.bb_embs, data.pc7], dim=1)
        x = torch.cat([x, data.side_chain_embs, data.bb_embs], dim=1)
        pos, seq, ori = data.pos, data.seq, data.ori
        batch = data.batch
        try:
            pos_n, pos_cb = data.pos_n, data.pos_cb

            # Forward through stacked BasicBlocks
            for i, layer in enumerate(self.layers):
                x = layer(x, pos, seq, ori, batch)
                if i == len(self.layers) - 1:
                    x = global_mean_pool(x, batch)
                elif i % 2 == 1:
                    x, pos, seq, ori, batch, pos_n, pos_cb = self.local_mean_pool(
                        x, pos, seq, ori, batch, pos_n, pos_cb
                    )
        except:
            for i, layer in enumerate(self.layers):
                x = layer(x, pos, seq, ori, batch)
                if i == len(self.layers) - 1:
                    x = global_mean_pool(x, batch)
                elif i % 2 == 1:
                    x, pos, seq, ori, batch = self.local_mean_pool1(
                        x, pos, seq, ori, batch
                    )
        return self.classifier(x)


class Model_ESM(nn.Module):
    def __init__(self,
                 geometric_radii: List[float],
                 sequential_kernel_size: float,
                 kernel_channels: List[int],
                 channels: List[int],
                 base_width: float = 16.0,
                 embedding_dim: int = 16,
                 batch_norm: bool = True,
                 dropout: float = 0.2,
                 bias: bool = False,
                 num_classes: int = 384) -> nn.Module:

        super().__init__()

        assert (len(geometric_radii) == len(channels)), "Model: 'geometric_radii' and 'channels' should have the same number of elements!"

        self.embedding = torch.nn.Embedding(num_embeddings=21, embedding_dim=embedding_dim)
        self.local_mean_pool = AvgPooling1()

        layers = []
        in_channels = embedding_dim + 8 + 6 + 1280
        for i, radius in enumerate(geometric_radii):
            layers.append(BasicBlock(r = radius,
                                     l = sequential_kernel_size,
                                     kernel_channels = kernel_channels,
                                     in_channels = in_channels,
                                     out_channels = channels[i],
                                     base_width = base_width,
                                     batch_norm = batch_norm,
                                     dropout = dropout,
                                     bias = bias))
            layers.append(BasicBlock(r = radius,
                                     l = sequential_kernel_size,
                                     kernel_channels = kernel_channels,
                                     in_channels = channels[i],
                                     out_channels = channels[i],
                                     base_width = base_width,
                                     batch_norm = batch_norm,
                                     dropout = dropout,
                                     bias = bias))
            in_channels = channels[i]

        self.layers = nn.Sequential(*layers)

        self.classifier = MLP(in_channels=channels[-1],
                              mid_channels=max(channels[-1], num_classes),
                              out_channels=num_classes,
                              batch_norm=batch_norm,
                              dropout=dropout)

    def forward(self, data):
        x, pos, seq, ori, batch, side_chain_embs, bb_embs, esm_emb = (self.embedding(data.x), data.pos, data.seq, data.ori, data.batch, data.side_chain_embs, data.bb_embs, data.esm_emb)

        x = torch.cat([x, side_chain_embs, bb_embs, esm_emb], dim=1)

        for i, layer in enumerate(self.layers):
            x = layer(x, pos, seq, ori, batch)
            if i == len(self.layers) - 1:
                x = global_mean_pool(x, batch)
            elif i % 2 == 1:
                x, pos, seq, ori, batch = self.local_mean_pool(x, pos, seq, ori, batch)

        out = self.classifier(x)

        return out